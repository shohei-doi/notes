---
title: "Rでロジスティック回帰"
description: |
  Rでロジスティック回帰を行う方法を説明します。
author:
  - name: 土井　翔平
    affiliation: 国立情報学研究所
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 2
categories:
  - R
  - data analysis
twitter:
  creator: "@sdoi0504"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  error = TRUE,
  warning = TRUE,
  message = TRUE,
  R.options = list(width = 100)
)
```

## はじめに {#Intro}

[前回](https://shohei-doi.github.io/notes/posts/2019-05-17-regression/)自民党の感情温度という一定の範囲内の値を取る数値を応答変数に持つ分析を行いました。
しかし、世の中のデータのすべてがこのように数値で表現できるとは限りません。

例えば「投票に行く（行かない）」や「投票した政党」のように数値化されていないデータをカテゴリカルデータ（変数）と呼びます。
このような場合を応答変数にする分析の場合は、「投票に行く確率」や「ある政党に投票する確率」を求めることで対処します。

以下では、応答変数の種類が2種類の場合と3以上の場合に分けて、カテゴリカルデータの分析をRで行う方法を紹介します。

### 必要なパッケージの読み込み

```{r}
library(tidyverse)
```

多項ロジットを行うためのライブラリーを読み込みます。

```{r}
library(nnet)
```

### データの読み込み

[東大・朝日共同調査](http://www.masaki.j.u-tokyo.ac.jp/utas/utasv.html)の2014年衆院選・2016年参院選世論調査のデータを使います。

```{r}
data <- read_csv("http://www.masaki.j.u-tokyo.ac.jp/utas/2014_2016UTASV20161004.csv", 
                 locale = locale(encoding = "shift-jis"), na = c("66", "99", "999"))
```

- 非該当は`66`で、無回答は`99`なので、これらを欠損値として読み込んでおきます。

なお、以下で特に断りのない限り、2014年の衆院選のデータを使います。

## 応答変数の種類が2種類の場合 {#Logit}

「投票に行く」あるいは「行かない」のように応答変数が2つの値のどちらかを取る場合を、まずは考えます。
選択肢は「投票に行く」か「行かない」しかないので、「投票に行く確率」を求めれば「投票に行かない確率」も分かる点に注意しましょう。

### 線形確率モデル

実は、最小二乗法をそのまま使うこともできます。
試しに投票率が年齢によって増えるかどうかを確認します。

```{r}
data <- data %>% 
  mutate(W1Q1 = W1Q1 - 1)
model1 <- lm(W1Q1 ~ W1F2, data = data)
summary(model1)
```

10歳増えるごとに投票率が5%上がることがわかります。

- `W1Q1`は投票に行くと`2`、行かないと`0`なので、`1`を引いています。

```{r}
data %>% 
  ggplot(aes(x = W1F2, y = W1Q1)) + 
  geom_jitter() + 
  geom_smooth(method = "lm")
```

### ロジスティック回帰の前提

線形確率モデルには良い点も悪い点もありますが、一般的にはロジスティック回帰（ロジット）を用いることが多いです。

大雑把に言うと、最小二乗法での式の右辺を

$$
  y_i = \beta_0 + \beta_1 X_{i1} + \cdots \beta_m X_{im} + \varepsilon_i
$$

ロジスティック関数あるいはシグモイド関数で変形します。」

$$
  y_i = \frac{e^{\beta_0 + \beta_1 X_{i1} + \cdots \beta_m X_{im} + \varepsilon_i}}{1+e^{\beta_0 + \beta_1 X_{i1} + \cdots \beta_m X_{im} + \varepsilon_i}}
$$

<aside>
  計量経済学では前者、機械学習では後者の名称がよく使われています。
</aside>

- $e^x$は$\exp(x)$と表されることもあります。

この操作により、$y_i$が0から1の間に収まるようになり、確率の定義に対応します。

```{r}
tibble(x = seq(-3,3,by = 0.1),
       y = plogis(x)) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_line()
```

### ロジスティック回帰でやっていること

アイデアは最小二乗法と変わりません。
予測値と観測値のズレを最小にするような$\beta_{j}$を求めるだけです。

専門的な言葉を使うと、機械学習ではクロスエントロピー誤差を最小化します。
計量経済学では、それを裏返して、対数尤度関数を最大化します。
表現は違えど、やっていることは同じです。

### Rによるロジスティック回帰

Rでロジスティック回帰をする場合、最小二乗法のときとそれほど変わりません。
おまじないは2つあり、1つ目は`lm()`の代わりに`glm()`を使うこと。
2つ目は`family = binomial()`というオプションをつけることです。

```{r}
model2 <- glm(W1Q1 ~ W1F2, family = binomial(), data = data)
summary(model2)
```

年齢とともに投票確率が上がるという結論は変わりませんが、線形確率モデルのときとは違い、10歳上がるごとに投票確率が20%上がるという解釈はできません。

```{r}
data %>% 
  ggplot(aes(x = W1F2, y = ,W1Q1)) + 
  geom_jitter() + 
  geom_smooth(method = "glm", method.args = list(family = "binomial"))
```

- 今回の例だと線形確率モデルと大差ないですね。

もちろん、最小二乗法と同様、複数の説明変数を入れたり、多項式や交差項を含めることが可能です。

### 予測精度

ロジスティック回帰の結果を使って、投票に行く確率を`predict()`で予測できます。

```{r}
predict(model2, type = "response") %>% 
  head()
```

それでは、予測結果を`predict`という変数として`data`内に作成します。
また、予測結果は0から1の間の確率ですが、観測値は投票に行ったかどうかの0もしくは1なので、予測確率が`0.5`以上の場合は投票に行くと予測して`vote`を`1`として上書きし、`0.5`以下であれば`0`とします。

```{r}
data <- data %>% 
  drop_na(W1F2) %>% 
  mutate(predict = predict(model2, type = "response"),
         vote = if_else(predict > 0.5, 1, 0))
```

- 年齢`W1F2`が欠損しているサンプルは分析で使われていないので除外します。
- `type = "response"`とすることで予測確率を計算します。

予測精度を測る指標はいくつかありますが、ここではシンプルに正答率でいきたいと思います。
つまり、「実際に投票に行ったかどうか」と「投票に行ったかどうかの予測」が一致したサンプルの数を全体で割ったものを計算します。

```{r}
sum(data$W1Q1 == data$vote)/nrow(data)
```

年齢だけで70%以上の人々の投票行動を予測することができました。

せっかくなので、投票に行った人だけに限定して予測精度を確認してみます。

```{r}
temp <- data %>% 
  filter(W1Q1 == 1)
sum(temp$W1Q1 == temp$vote)/nrow(temp)
```

投票に行った人を100%の確率で的中していることがわかりました（投票に行った人はすべて投票に行くと予測された）。

- もちろん、これにはからくりがあります。

## 応答変数の種類が3つ以上の場合 {#Multinomial}

続いて、「比例区でどの政党に投票したのか」という応答変数の取りうる値が3つ以上の場合を考えます。
やや複雑にはなりますが、「政党Aに投票する確率」を求めるという点では大きく変わりません。

細かい数学的な話は省きますが、シグモイド関数を拡張したソフトマックス関数で変形して、「$i$さんが$j$を選択する確率」を計算します。

$$
  y_{ij} = \frac{e^{\beta_0 + \beta_{1k} X_{i1} + \cdots \beta_{mk} X_{im} + \varepsilon_i}}{\sum_j e^{\beta_0 + \beta_{1k} X_{i1} + \cdots \beta_{mk} X_{im} + \varepsilon_i}}
$$

あとはクロスエントロピー誤差を最小化あるいは対数尤度関数を最大化して$\beta_{jk}$を求めます。

### Rで多項ロジット

Rで多項ロジットをする場合、`nnet`や`mlogit`といったパッケージを利用します。
今回は簡単な`nnet`を使います（統計的仮説検定をしたい場合は`mlogit`の方が良さそうです）。

これまで1`lm()`や`glm()`だったところを`multinom()`に変えるだけです。

```{r}
model3 <- multinom(W1Q2 ~ W1F2, data = data)
summary(model3)
```

### 予測精度

ロジットのときと同様、予測した結果と実際の投票先の的中率を求めてみます。
今回は`predict()`で投票先をそのまま予測してくれるので、それを`party`という変数にします。

```{r}
data <- data %>% 
  drop_na(W1Q2) %>% 
  mutate(party = predict(model3))
sum(data$W1Q2 == data$party)/nrow(data)
```

- 比例区で投票先が欠損しているサンプルを除外しています。

的中率は35%でした。

- これが高いと見るか低いと見るかはともかく、ここでもとあるからくりがあります。